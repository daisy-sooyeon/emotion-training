{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNKbFVpSn52z",
        "outputId": "445846b4-c9ba-44f5-8cc2-ab22e422003d"
      },
      "outputs": [],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVrRgZ3fVnUA"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 데이터 확인\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset 만들기\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Detect Face\n",
        "import cv2\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# Importing necessary layers and modules from TensorFlow instead of standalone Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jObdDsxTYaXi"
      },
      "outputs": [],
      "source": [
        "# gpu사용 변수\n",
        "import tensorflow as tf\n",
        "\n",
        "device = tf.device('cuda:3')\n",
        "# device = torch.device('cuda:3') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGeG9TiAYmTL",
        "outputId": "634c60b8-c957-4802-8bed-d163edc4f79d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okfcWJNtPBsT"
      },
      "source": [
        "### ------------------- 훈련 데이터프레임 생성하기 -------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE16Oqg62nFS"
      },
      "outputs": [],
      "source": [
        "# 전체 이미지에서 얼굴을 찾아내는 함수\n",
        "def detect_face(frame):\n",
        "\n",
        "    # cascade pre-trained 모델 불러오기\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # RGB를 gray scale로 바꾸기\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # cascade 멀티스케일 분류\n",
        "    detected_faces = face_cascade.detectMultiScale(gray,\n",
        "                                                   scaleFactor = 1.1,\n",
        "                                                   minNeighbors = 6,\n",
        "                                                   minSize = (shape_x, shape_y),\n",
        "                                                   flags = cv2.CASCADE_SCALE_IMAGE\n",
        "                                                  )\n",
        "\n",
        "    coord = []\n",
        "    for x, y, w, h in detected_faces:\n",
        "        if w > 100:\n",
        "            sub_img = frame[y:y+h, x:x+w]\n",
        "            coord.append([x, y, w, h])\n",
        "\n",
        "    return gray, detected_faces, coord\n",
        "\n",
        "# 전체 이미지에서 찾아낸 얼굴을 추출하는 함수\n",
        "def extract_face_features(gray, detected_faces, coord, offset_coefficients=(0.075, 0.05)):\n",
        "    new_face = []\n",
        "    for det in detected_faces:\n",
        "\n",
        "        # 얼굴로 감지된 영역\n",
        "        x, y, w, h = det\n",
        "\n",
        "        # 이미지 경계값 받기\n",
        "        horizontal_offset = int(np.floor(offset_coefficients[0] * w))\n",
        "        vertical_offset = int(np.floor(offset_coefficients[1] * h))\n",
        "\n",
        "        # gray scacle 에서 해당 위치 가져오기\n",
        "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
        "\n",
        "        # 얼굴 이미지만 확대\n",
        "        new_extracted_face = zoom(extracted_face, (shape_x/extracted_face.shape[0], shape_y/extracted_face.shape[1]))\n",
        "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
        "        new_extracted_face /= float(new_extracted_face.max()) # sacled\n",
        "        new_face.append(new_extracted_face)\n",
        "\n",
        "    return new_face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sKAk4Bi4N3Xl",
        "outputId": "192932bd-a585-44f6-e0c8-e42317849d89"
      },
      "outputs": [],
      "source": [
        "origin_data = pd.read_csv('/content/drive/MyDrive/dataset/fer2013.csv')\n",
        "origin_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQQEmevbSYOz",
        "outputId": "7d874d82-2e3a-4963-8d3a-74e512a16b1a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2  # OpenCV 라이브러리 사용\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 각 감정 유형의 디렉토리 이름과 해당하는 레이블\n",
        "emotions = {\n",
        "    'angry': 0,\n",
        "    'disgust': 1,\n",
        "    'happy': 2,\n",
        "    'sad': 3,\n",
        "    'neutral': 4\n",
        "}\n",
        "\n",
        "shape_x = 96\n",
        "shape_y = 96\n",
        "\n",
        "# 기본 디렉토리 경로\n",
        "base_dir = \"/content/drive/MyDrive/Images/images_Yuri\"\n",
        "base_dir_2 = \"/content/drive/MyDrive/Images/images_swan\"\n",
        "\n",
        "# 원본 데이터 로드\n",
        "origin_data = pd.read_csv('/content/drive/MyDrive/dataset/fer2013.csv')\n",
        "\n",
        "# emotion 컬럼에서 2와 5를 가진 행 제거\n",
        "origin_data = origin_data[~origin_data['emotion'].isin([2, 5])]\n",
        "\n",
        "# emotion 컬럼 값 변경: 3 -> 2, 4 -> 3, 6 -> 4\n",
        "origin_data['emotion'] = origin_data['emotion'].replace({\n",
        "    3: 2,\n",
        "    4: 3,\n",
        "    6: 4\n",
        "})\n",
        "\n",
        "# 이미지를 지정된 크기로 리사이징하는 함수\n",
        "def resize_image(filepath, new_width=shape_x, new_height=shape_y):\n",
        "    face = cv2.imread(filepath)\n",
        "    if face is None:  # 이미지 파일이 없거나 읽을 수 없으면 None 반환\n",
        "        return None\n",
        "\n",
        "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)  # 흑백으로 변환\n",
        "    resized_face = cv2.resize(face, (new_width, new_height))  # 크기 조정\n",
        "    return resized_face.flatten()  # 평탄화하여 반환\n",
        "\n",
        "# 데이터프레임 생성 함수\n",
        "def create_emotion_df(base_dir):\n",
        "    data = {'emotion': [], 'pixels': [], 'Usage': []}\n",
        "\n",
        "    for emotion, label in emotions.items():\n",
        "        image_dir = os.path.join(base_dir, emotion)\n",
        "        image_files = [\n",
        "            os.path.join(image_dir, file)\n",
        "            for file in os.listdir(image_dir)\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "        ]\n",
        "        for filepath in image_files:\n",
        "            pixels = resize_image(filepath)  # 이미지 처리\n",
        "            if pixels is None:  # None 값 건너뛰기\n",
        "                continue\n",
        "            data['emotion'].append(label)\n",
        "            data['pixels'].append(pixels)  # 평탄화된 픽셀 추가\n",
        "            data['Usage'].append('Training')\n",
        "\n",
        "    # 데이터프레임 생성\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Train/Test Split\n",
        "    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "    train_df['Usage'] = 'Training'\n",
        "    test_df['Usage'] = 'PublicTest'\n",
        "\n",
        "    return pd.concat([train_df, test_df])\n",
        "\n",
        "# 데이터프레임 생성\n",
        "final_df = create_emotion_df(base_dir)\n",
        "final_df_2 = create_emotion_df(base_dir_2)\n",
        "\n",
        "# pixels 데이터를 공백으로 구분된 문자열로 변환\n",
        "def flatten_pixels(df):\n",
        "    df['pixels'] = df['pixels'].apply(lambda x: ' '.join(map(str, x)))\n",
        "    return df\n",
        "\n",
        "# pixels 열 변환\n",
        "final_df = flatten_pixels(final_df)\n",
        "final_df_2 = flatten_pixels(final_df_2)\n",
        "\n",
        "# 원본 데이터와 병합\n",
        "final_df = pd.concat([final_df, final_df_2, origin_data], ignore_index=True)\n",
        "\n",
        "# None 값 제거\n",
        "final_df = final_df[final_df['pixels'].apply(lambda x: x is not None)]\n",
        "\n",
        "# 데이터 확인\n",
        "print(final_df.head())\n",
        "print(final_df['pixels'].iloc[0])  # 첫 번째 픽셀 데이터 확인\n",
        "print(f\"Total samples: {len(final_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkrTXW7VQ9hZ",
        "outputId": "fbcf0407-2668-47a9-a518-dd092e07cbba"
      },
      "outputs": [],
      "source": [
        "len(final_df['pixels'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpiHPAWSVgXP",
        "outputId": "2f8724be-bb82-40de-ba27-4b81ead03c16"
      },
      "outputs": [],
      "source": [
        "# 각 샘플의 픽셀 데이터 크기 확인\n",
        "final_df['pixel_length'] = final_df['pixels'].apply(lambda x: len(x.split()) if isinstance(x, str) else len(x))\n",
        "print(final_df['pixel_length'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez_JfQmeVXEm",
        "outputId": "ccab48a4-7f9b-429c-b992-3632d7aa92d0"
      },
      "outputs": [],
      "source": [
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "\n",
        "# 픽셀 데이터 크기 변환 함수\n",
        "def standardize_pixels(pixel_data, current_length, target_size=(96, 96)):\n",
        "    # 문자열을 numpy 배열로 변환\n",
        "    if isinstance(pixel_data, str):\n",
        "        flat_array = np.array(list(map(float, pixel_data.split())))\n",
        "    else:\n",
        "        flat_array = np.array(pixel_data)\n",
        "\n",
        "    # 현재 데이터 크기를 계산\n",
        "    current_size = int(np.sqrt(current_length))  # ex: 2304 -> 48, 9216 -> 96\n",
        "    if current_size * current_size != current_length:\n",
        "        raise ValueError(\"Invalid pixel data size: not a perfect square\")\n",
        "\n",
        "    # (current_size, current_size)로 리쉐이프\n",
        "    pixel_array = flat_array.reshape((current_size, current_size))\n",
        "\n",
        "    # 지정된 크기로 리사이즈\n",
        "    resized_array = resize(pixel_array, target_size, anti_aliasing=True)\n",
        "    return resized_array.flatten()  # 다시 1차원으로 변환\n",
        "\n",
        "# 데이터 변환 함수 적용\n",
        "def resize_dataset(df):\n",
        "    df['pixels'] = df.apply(\n",
        "        lambda row: standardize_pixels(row['pixels'], row['pixel_length']), axis=1\n",
        "    )\n",
        "    # (96, 96, 1)로 리쉐이프\n",
        "    df['pixels'] = df['pixels'].apply(lambda x: np.array(x).reshape((96, 96, 1)))\n",
        "    return df\n",
        "\n",
        "# 데이터프레임에 변환 적용\n",
        "final_df = resize_dataset(final_df)\n",
        "\n",
        "# 변환된 데이터 검증\n",
        "print(final_df['pixels'].iloc[0].shape)  # 출력: (96, 96, 1)\n",
        "print(final_df['pixels'].apply(lambda x: x.size).value_counts())  # 출력: 9216"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVUVP7cAQ8vj"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터와 테스트 데이터 분할\n",
        "train_df = final_df[final_df['Usage'] == 'Training']\n",
        "test_df = final_df[final_df['Usage'] == 'PublicTest']\n",
        "\n",
        "# 데이터 준비\n",
        "X_train = np.stack(train_df['pixels'].values)\n",
        "y_train = train_df['emotion'].values\n",
        "X_test = np.stack(test_df['pixels'].values)\n",
        "y_test = test_df['emotion'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWvKZ1WGTWxH",
        "outputId": "059cee1c-26ee-459e-c4cd-fe4af8e6f8e0"
      },
      "outputs": [],
      "source": [
        "# 데이터 크기와 샘플 확인\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(X_train[0])  # 첫 번째 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SU1vVcnqYybG",
        "outputId": "bdaebf08-69b5-4057-9140-785027015ba9"
      },
      "outputs": [],
      "source": [
        "# 4차원 데이터셋 만들기 (데이터개수, x축, y축, rgb)\n",
        "X_train_ds = np.reshape(X_train, (X_train.shape[0], shape_x, shape_y, 1))\n",
        "y_train_ds = np.reshape(y_train, (y_train.shape[0], 1))\n",
        "\n",
        "X_test_ds = np.reshape(X_test, (X_test.shape[0], shape_x, shape_y, 1))\n",
        "y_test_ds = np.reshape(y_test, (y_test.shape[0], 1))\n",
        "\n",
        "print(X_train_ds.shape, y_train_ds.shape)\n",
        "print(X_test_ds.shape, y_test_ds.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHpbOXpEYydX",
        "outputId": "b8730164-11d3-4c4c-dd10-5b3fedba1410"
      },
      "outputs": [],
      "source": [
        "# 데이터타입 float로 변경\n",
        "train_data = X_train_ds.astype('float32')\n",
        "test_data = X_test_ds.astype('float32')\n",
        "\n",
        "# 스케일링\n",
        "train_data /= 225\n",
        "test_data /= 225\n",
        "\n",
        "# y데이터 원핫인코딩\n",
        "train_labels_onehot = to_categorical(y_train_ds)\n",
        "test_labels_onehot = to_categorical(y_test_ds)\n",
        "\n",
        "# input_shape 설정\n",
        "n_rows, n_cols, n_dims = X_train_ds.shape[1:]\n",
        "input_shape = (n_rows, n_cols, n_dims)\n",
        "print(input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "5FPLqVGpYyf7",
        "outputId": "8121f70f-ad36-4501-bb7f-dd3ec40fd09a"
      },
      "outputs": [],
      "source": [
        "# 라벨 숫자를 문자로 변경\n",
        "def get_label(argument):\n",
        "    labels = {0:'angry', 1:'disgust', 2:'happy', 3:'sad', 4:'neutral'}\n",
        "    return(labels.get(argument, 'Invalid emotion'))\n",
        "\n",
        "# 데이터 시각화\n",
        "plt.figure(figsize=[10,5])\n",
        "\n",
        "# Train data 중 100번째 이미지\n",
        "n=150\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.imshow(np.squeeze(X_train_ds[n,:,:], axis = 2), cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(get_label(int(y_train[n]))))\n",
        "\n",
        "# Test data 중 100번째 이미지\n",
        "plt.subplot(122)\n",
        "plt.imshow(np.squeeze(X_test_ds[n,:,:], axis = 2), cmap='gray')\n",
        "plt.title(\"Ground Truth : {}\".format(get_label(int(y_test[n]))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14ZHLGzKasJa"
      },
      "outputs": [],
      "source": [
        "def simple_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
        "\n",
        "    # Add layers\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Flatten\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "\n",
        "    # Output layer : n_classes=5\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "cCE3SZ9oa0h9",
        "outputId": "7442c41c-85ca-4994-e5b0-b8ddab726e38"
      },
      "outputs": [],
      "source": [
        "model = simple_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUB3cSW7dp_x",
        "outputId": "dcdf3ae5-3ac4-47d2-cd14-da77bcf715d7"
      },
      "outputs": [],
      "source": [
        "# 학습\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 이미지 데이터 증강\n",
        "datagen = ImageDataGenerator(zoom_range=0.2,          # 랜덤하게 이미지 줌 하는 비율\n",
        "                             rotation_range=10,       # 램덤하게 이미지 회전하는 비율 (0도~180도)\n",
        "                             width_shift_range=0.1,   # 랜덤하게 이미지 가로로 이동하는 비율\n",
        "                             height_shift_range=0.1,  # 랜덤하게 이미지 세로로 이동하는 비율\n",
        "                             horizontal_flip=True,    # 랜덤하게 이미지 수평 뒤집기\n",
        "                             vertical_flip=False)     # 랜덤하게 이미지 수직 뒤집기\n",
        "\n",
        "# 모델 학습을 위한 파라미터 설정\n",
        "batch_size = 32\n",
        "n_epochs = 10\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history = model.fit_generator(datagen.flow(train_data, train_labels_onehot, batch_size=batch_size),\n",
        "#                               steps_per_epoch=int(np.ceil(train_data.shape[0]/float(batch_size))),\n",
        "#                               epochs=n_epochs,\n",
        "#                               validation_data=(test_data, test_labels_onehot)\n",
        "#                              )\n",
        "\n",
        "history = model.fit(\n",
        "    datagen.flow(train_data, train_labels_onehot, batch_size=batch_size),\n",
        "    steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))),\n",
        "    epochs=n_epochs,\n",
        "    validation_data=(test_data, test_labels_onehot)  # Ensure this is included\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "7Nx8gM5NdsuC",
        "outputId": "86915366-5913-4002-a80a-4da77f3175fa"
      },
      "outputs": [],
      "source": [
        "# Loss Curves\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['loss'],'r',linewidth=2.0)\n",
        "plt.plot(history.history['val_loss'],'b',linewidth=2.0)\n",
        "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Loss',fontsize=16)\n",
        "plt.title('Loss Curves',fontsize=16)\n",
        "\n",
        "# Accuracy Curves\n",
        "plt.figure(figsize=[8,6])\n",
        "plt.plot(history.history['accuracy'],'r',linewidth=2.0)\n",
        "plt.plot(history.history['val_accuracy'],'b',linewidth=2.0)\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
        "plt.xlabel('Epochs ',fontsize=16)\n",
        "plt.ylabel('Accuracy',fontsize=16)\n",
        "plt.title('Accuracy Curves',fontsize=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTXElBOR_tL8"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/Model/sad.keras'  # Update the path as needed\n",
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "hLVGiNwT_Fm5",
        "outputId": "73d247ae-c769-4dbf-dfc9-1db218faea41"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/dataset/disgust_6.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(len(output_data[0]))\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "gSr44Plm_FqF",
        "outputId": "c2db3130-4d33-4b64-b58d-62219db9af9b"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/Images/images_Yuri/disgust/IMG_2584.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(\"angry :\", output_data[0][0], \"disgust :\", output_data[0][1], \"happy :\", output_data[0][2], \"sad :\", output_data[0][3], \"neutral :\", output_data[0][4])\n",
        "\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "Jgz1EKGmYqqk",
        "outputId": "4e47d7ab-0755-48a4-e5ea-a5fe05a25cf5"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/Images/images_tmddus/happy/image_412.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(\"angry :\", output_data[0][0], \"disgust :\", output_data[0][1], \"happy :\", output_data[0][2], \"sad :\", output_data[0][3], \"neutral :\", output_data[0][4])\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "CQAUlEmSYtXM",
        "outputId": "5f883c65-547f-4b82-91d3-f9ba25d5c4fd"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/Images/images_tmddus/happy/image_501.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(\"angry :\", output_data[0][0], \"disgust :\", output_data[0][1], \"happy :\", output_data[0][2], \"sad :\", output_data[0][3], \"neutral :\", output_data[0][4])\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "UvV8DaS1Y3Dt",
        "outputId": "93481c07-1e15-45aa-8d04-50207d8c667c"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/Images/images_soo/happy/image_110.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(\"angry :\", output_data[0][0], \"disgust :\", output_data[0][1], \"happy :\", output_data[0][2], \"sad :\", output_data[0][3], \"neutral :\", output_data[0][4])\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "n8sPN8oqY8w5",
        "outputId": "153d39d8-4c66-4e38-953f-0b33679c79b8"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/Images/images_swan/happy/image_300.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(\"angry :\", output_data[0][0], \"disgust :\", output_data[0][1], \"happy :\", output_data[0][2], \"sad :\", output_data[0][3], \"neutral :\", output_data[0][4])\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "AjjtT8SgZGdM",
        "outputId": "2493e0e7-70c4-4941-b46d-172f9357b9ab"
      },
      "outputs": [],
      "source": [
        "# 원본이미지 확인\n",
        "face = cv2.imread('/content/drive/MyDrive/Images/images_Yuri/happy/angry_651.jpg')\n",
        "\n",
        "# 얼굴 추출\n",
        "gray, detected_faces, coord = detect_face(face)\n",
        "face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "# 모델 추론\n",
        "input_data = np.reshape(face_zoom[0].flatten(), (1, 96, 96, 1))\n",
        "output_data = model.predict(input_data)\n",
        "print(\"angry :\", output_data[0][0], \"disgust :\", output_data[0][1], \"happy :\", output_data[0][2], \"sad :\", output_data[0][3], \"neutral :\", output_data[0][4])\n",
        "\n",
        "result = np.argmax(output_data)\n",
        "\n",
        "# 결과 문자로 변환\n",
        "if result == 0:\n",
        "    emotion = 'angry'\n",
        "elif result == 1:\n",
        "    emotion = 'disgust'\n",
        "elif result == 2:\n",
        "    emotion = 'happy'\n",
        "elif result == 3:\n",
        "    emotion = 'sad'\n",
        "elif result == 4:\n",
        "    emotion = 'neutral'\n",
        "\n",
        "# 시각화\n",
        "plt.subplot(121)\n",
        "plt.title(\"Original Face\")\n",
        "plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title(f\"Extracted Face : {emotion}\")\n",
        "plt.imshow(face_zoom[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "qSYAevMtZLKx",
        "outputId": "7521eeda-52b7-4095-fbb0-6d6aa79c9d2a"
      },
      "outputs": [],
      "source": [
        "# real-time video demo\n",
        "\n",
        "import cv2\n",
        "path = '/Users/daisy/Desktop/2022-13750/3학년 2학기/감정트레이닝/haarcascade_frontalface_default.xml'\n",
        "font_scale = 1.5\n",
        "font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "rectangle_bgr = (255, 255, 255)\n",
        "img = np.array((500, 500))\n",
        "\n",
        "text = 'Some text in the box'\n",
        "(text_width, text_height) = cv2.getTextSize(text, font, fontScale = font_scale, thickness = 1)[0]\n",
        "text_offset_x = 10\n",
        "text_offset_y = img.shape[0] - 25\n",
        "\n",
        "box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))\n",
        "cv2.rectangle(img, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
        "cv2.putText(img, text, (text_offset_x, text_offset_y), font, fontScale=font_scale, color = (0, 0, 0), thickness=1)\n",
        "\n",
        "cap = cv2.VideoCapture(1)\n",
        "if not cap.isOpened():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "if not cap.isOpened():\n",
        "    raise IOError('Cannot open webcam')\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + path)\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
        "    for x, y, w, h in faces:\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_color = frame[y:y+h, x:x+w]\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        facess = faceCascade.detectMultiScale(roi_gray)\n",
        "        if len(facess) == 0:\n",
        "            print('Face not detected')\n",
        "        else:\n",
        "            for (ex, ey, ew, eh) in facess:\n",
        "                face_roi = roi_color[ey:ey+eh, ex:ex+ew]\n",
        "\n",
        "    final_image = cv2.resize(face_roi, (224, 224))\n",
        "    final_image = np.expand_dims(final_image, axis = 0)\n",
        "    final_image = final_image / 255.0\n",
        "\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "    Predictions = new_model.predict(final_image)\n",
        "\n",
        "    font_scale = 1.5\n",
        "    font = cv2.FONT_HERSHEY_PLAIN\n",
        "\n",
        "    # 여기서부터 다양한 감정으로 조정하면 될 것 같음\n",
        "    if (np.argmax(Predictions) == 0):\n",
        "        status = 'Angry'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "    elif (np.argmax(Predictions)==1):\n",
        "        status = 'Disgust'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "    elif (np.argmax(Predictions)==2):\n",
        "        status = 'Fear'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "    elif (np.argmax(Predictions)==3):\n",
        "        status = 'Happy'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "    elif (np.argmax(Predictions)==4):\n",
        "        status = 'Sad'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "\n",
        "    elif (np.argmax(Predictions)==5):\n",
        "        status = 'Surprise'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "    else:\n",
        "        status = 'Neutral'\n",
        "\n",
        "        x1, y1, w1, h1 = 0, 0, 175, 75\n",
        "        cv2.rectangle(frame, (x1, x1), (x1+w1, y1+h1), (0, 0, 0), -1)\n",
        "        cv2.putText(frame, status, (x1+int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
        "        cv2.putText(frame, status, (100, 150), font, 3, (0, 0, 255), 2, cv2.LINE_4)\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 0, 255))\n",
        "\n",
        "    cv2.imshow('Face Emotion Recognition', frame)\n",
        "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SkUxav5In8y"
      },
      "outputs": [],
      "source": [
        "pip install opencv-python\n",
        "pip install tensorflow  # 또는 keras, 모델에 따라 다름"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOKs_aYIIqaw"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# 모델 로드\n",
        "model_path = '/content/drive/MyDrive/Model/tmddus.keras'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# 얼굴 검출을 위한 OpenCV 전처리\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# 감정 레이블\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "def get_face(frame):\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
        "    if len(faces) == 0:\n",
        "        return None, None\n",
        "    (x, y, w, h) = faces[0]\n",
        "    return gray[y:y+h, x:x+w], faces[0]\n",
        "\n",
        "def analyze_emotion(face_image):\n",
        "    face_image = cv2.resize(face_image, (48, 48))\n",
        "    face_image = face_image.reshape((1, 48, 48, 1))\n",
        "    face_image = face_image.astype('float32') / 255\n",
        "    return model.predict(face_image)\n",
        "\n",
        "# 카메라 캡처 시작\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    face, bbox = get_face(frame)\n",
        "    if face is not None:\n",
        "        output_data = analyze_emotion(face)\n",
        "        emotion = emotion_labels[np.argmax(output_data)]\n",
        "\n",
        "        # 결과 표시\n",
        "        x, y, w, h = bbox\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "\n",
        "    cv2.imshow('Video Feed', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS6X7k5Eko84"
      },
      "source": [
        "# ---------------------- test -----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOrUBmNulv31"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 데이터 확인\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset 만들기\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Detect Face\n",
        "import cv2\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# Importing necessary layers and modules from TensorFlow instead of standalone Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGT9QqwdkrqD",
        "outputId": "dc24c2c2-1c42-4488-b371-9b541498be96"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfJXcP08lf17"
      },
      "outputs": [],
      "source": [
        "# 전체 이미지에서 얼굴을 찾아내는 함수\n",
        "def detect_face(frame):\n",
        "\n",
        "    # cascade pre-trained 모델 불러오기\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # RGB를 gray scale로 바꾸기\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # cascade 멀티스케일 분류\n",
        "    detected_faces = face_cascade.detectMultiScale(gray,\n",
        "                                                   scaleFactor = 1.1,\n",
        "                                                   minNeighbors = 6,\n",
        "                                                   minSize = (shape_x, shape_y),\n",
        "                                                   flags = cv2.CASCADE_SCALE_IMAGE\n",
        "                                                  )\n",
        "\n",
        "    coord = []\n",
        "    for x, y, w, h in detected_faces:\n",
        "        if w > 100:\n",
        "            sub_img = frame[y:y+h, x:x+w]\n",
        "            coord.append([x, y, w, h])\n",
        "\n",
        "    return gray, detected_faces, coord\n",
        "\n",
        "# 전체 이미지에서 찾아낸 얼굴을 추출하는 함수\n",
        "def extract_face_features(gray, detected_faces, coord, offset_coefficients=(0.075, 0.05)):\n",
        "    new_face = []\n",
        "    for det in detected_faces:\n",
        "\n",
        "        # 얼굴로 감지된 영역\n",
        "        x, y, w, h = det\n",
        "\n",
        "        # 이미지 경계값 받기\n",
        "        horizontal_offset = int(np.floor(offset_coefficients[0] * w))\n",
        "        vertical_offset = int(np.floor(offset_coefficients[1] * h))\n",
        "\n",
        "        # gray scacle 에서 해당 위치 가져오기\n",
        "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
        "\n",
        "        # 얼굴 이미지만 확대\n",
        "        new_extracted_face = zoom(extracted_face, (shape_x/extracted_face.shape[0], shape_y/extracted_face.shape[1]))\n",
        "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
        "        new_extracted_face /= float(new_extracted_face.max()) # sacled\n",
        "        new_face.append(new_extracted_face)\n",
        "\n",
        "    return new_face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdqvt7l4lgRD"
      },
      "outputs": [],
      "source": [
        "shape_x = 96\n",
        "shape_y = 96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "liEvbB61ktcP",
        "outputId": "e494b037-72f0-4bb5-d0ea-bde360d0dda8"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# 모델 로드\n",
        "model_path = '/content/drive/MyDrive/Model/sad.keras'  # Update the path as needed\n",
        "model = load_model(model_path)\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Resize the output to fit the video element.\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            // Wait for Capture to be clicked.\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "filename = take_photo()\n",
        "print(\"Image saved to:\", filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "8rVGZhk5kzP1",
        "outputId": "55be078a-b7bf-4e0f-e4df-903e6dd327d2"
      },
      "outputs": [],
      "source": [
        "emotion_labels = ['angry', 'disgust', 'happy', 'sad', 'neutral']  # Ensure this aligns with the model's output\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Model/sad.keras'  # Update the path as needed\n",
        "model = load_model(model_path)\n",
        "\n",
        "def analyze_emotion(image_path, model):\n",
        "    # Load the original image\n",
        "    face = cv2.imread(image_path)\n",
        "\n",
        "    # Extract face\n",
        "    gray, detected_faces, coord = detect_face(face)\n",
        "    if len(detected_faces) == 0:\n",
        "        print(\"No face detected in the image.\")\n",
        "        return\n",
        "    face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "    # Convert the extracted face into the format required by the model\n",
        "    img_array = np.reshape(face_zoom[0], (1, 96, 96, 1))\n",
        "\n",
        "    # Model prediction\n",
        "    prediction = model.predict(img_array)[0]  # Get the probabilities for each emotion\n",
        "\n",
        "    print(prediction)\n",
        "\n",
        "    # Convert prediction to percentages\n",
        "    prediction_percentages = {emotion_labels[i]: round(prob * 100, 10) for i, prob in enumerate(prediction)}\n",
        "\n",
        "    # Print the probabilities for each emotion\n",
        "    print(\"\\nPrediction Percentages:\")\n",
        "    for emotion, percentage in prediction_percentages.items():\n",
        "        print(f\"{emotion}: {percentage}%\")\n",
        "\n",
        "    # Display the original and extracted face\n",
        "    plt.subplot(121)\n",
        "    plt.title(\"Original Face\")\n",
        "    plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.title(\"Extracted Face\")\n",
        "    plt.imshow(face_zoom[0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "analyze_emotion(filename, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgK1Tjjik33j",
        "outputId": "b776de5d-c6d0-47c6-8d19-a5ea0f10c0fb"
      },
      "outputs": [],
      "source": [
        "print(f\"Min pixel value: {face_zoom[0].min()}, Max pixel value: {face_zoom[0].max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV8XAPuhlQnt",
        "outputId": "93d79c3e-a386-4254-cc89-f3faa6986b49"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Emotion labels\n",
        "emotion_labels = ['angry', 'disgust', 'happy', 'sad', 'neutral']\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_path = '/content/drive/MyDrive/Model/sad.keras'  # 모델 경로\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Define the base directory for the folders\n",
        "base_dir = \"/content/drive/MyDrive/Images/images_sohn\"  # 폴더 경로\n",
        "\n",
        "# Function to process a single image and get predictions\n",
        "def get_emotion_prediction(image_path, model):\n",
        "    # Load the image\n",
        "    face = cv2.imread(image_path)\n",
        "    if face is None:\n",
        "        print(f\"Error loading image: {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Extract the face\n",
        "    gray, detected_faces, coord = detect_face(face)\n",
        "    if len(detected_faces) == 0:\n",
        "        print(f\"No face detected in: {image_path}\")\n",
        "        return None\n",
        "    face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "    if face_zoom[0].shape != (96, 96):\n",
        "        face_zoom[0] = cv2.resize(face_zoom[0], (96, 96))  # Ensure size is correct\n",
        "\n",
        "    # Prepare the image for the model\n",
        "    img_array = np.reshape(face_zoom[0], (1, 96, 96, 1)).astype('float32') / 255.0\n",
        "\n",
        "    # Predict the emotion probabilities\n",
        "    prediction = model.predict(img_array)[0]  # Shape: (5,)\n",
        "    return prediction\n",
        "\n",
        "# Dictionary to store predictions for each folder\n",
        "emotion_predictions = {label: [] for label in emotion_labels}\n",
        "\n",
        "# Loop through each \"extra_\" folder\n",
        "for folder_name in os.listdir(base_dir):\n",
        "    if not folder_name.startswith(\"241120_\"):  # Only process folders starting with \"extra_\"\n",
        "        continue\n",
        "\n",
        "    folder_path = os.path.join(base_dir, folder_name)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    # Process each image in the folder\n",
        "    for image_file in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        prediction = get_emotion_prediction(image_path, model)\n",
        "        if prediction is not None:\n",
        "            emotion_predictions[folder_name.split('_')[-1]].append(prediction)  # Save prediction\n",
        "\n",
        "# Calculate the variance sum for each emotion\n",
        "variance_sum = {}\n",
        "for emotion, predictions in emotion_predictions.items():\n",
        "    if len(predictions) > 0:\n",
        "        predictions = np.array(predictions)  # Convert to NumPy array\n",
        "        variance_sum[emotion] = np.sum(np.var(predictions, axis=0))  # Variance sum across predictions\n",
        "\n",
        "# Print the variance sum for each emotion\n",
        "print(\"\\nVariance Sum for Each Emotion:\")\n",
        "for emotion, var_sum in variance_sum.items():\n",
        "    print(f\"{emotion}: {var_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtcUTJQHpTkM",
        "outputId": "f5229443-afc9-493c-c7f3-be83d3bace39"
      },
      "outputs": [],
      "source": [
        "total_variance_sum = sum(variance_sum.values())\n",
        "print('Total variance:', total_variance_sum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRbwX67lkGmN"
      },
      "source": [
        "# 완성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHCWNahkqrGY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hODnBcXkKTF"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 데이터 확인\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dataset 만들기\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Detect Face\n",
        "import cv2\n",
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "# Importing necessary layers and modules from TensorFlow instead of standalone Keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# 전체 이미지에서 얼굴을 찾아내는 함수\n",
        "def detect_face(frame):\n",
        "\n",
        "    # cascade pre-trained 모델 불러오기\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # RGB를 gray scale로 바꾸기\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # cascade 멀티스케일 분류\n",
        "    detected_faces = face_cascade.detectMultiScale(gray,\n",
        "                                                   scaleFactor = 1.1,\n",
        "                                                   minNeighbors = 6,\n",
        "                                                   minSize = (shape_x, shape_y),\n",
        "                                                   flags = cv2.CASCADE_SCALE_IMAGE\n",
        "                                                  )\n",
        "\n",
        "    coord = []\n",
        "    for x, y, w, h in detected_faces:\n",
        "        if w > 100:\n",
        "            sub_img = frame[y:y+h, x:x+w]\n",
        "            coord.append([x, y, w, h])\n",
        "\n",
        "    return gray, detected_faces, coord\n",
        "\n",
        "# 전체 이미지에서 찾아낸 얼굴을 추출하는 함수\n",
        "def extract_face_features(gray, detected_faces, coord, offset_coefficients=(0.075, 0.05)):\n",
        "    new_face = []\n",
        "    for det in detected_faces:\n",
        "\n",
        "        # 얼굴로 감지된 영역\n",
        "        x, y, w, h = det\n",
        "\n",
        "        # 이미지 경계값 받기\n",
        "        horizontal_offset = int(np.floor(offset_coefficients[0] * w))\n",
        "        vertical_offset = int(np.floor(offset_coefficients[1] * h))\n",
        "\n",
        "        # gray scacle 에서 해당 위치 가져오기\n",
        "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
        "\n",
        "        # 얼굴 이미지만 확대\n",
        "        new_extracted_face = zoom(extracted_face, (shape_x/extracted_face.shape[0], shape_y/extracted_face.shape[1]))\n",
        "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
        "        new_extracted_face /= float(new_extracted_face.max()) # sacled\n",
        "        new_face.append(new_extracted_face)\n",
        "\n",
        "    return new_face\n",
        "\n",
        "shape_x = 96\n",
        "shape_y = 96\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# 모델 로드\n",
        "model_path = '/content/drive/MyDrive/Model/sad.keras'  # Update the path as needed\n",
        "model = load_model(model_path)\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            // Resize the output to fit the video element.\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            // Wait for Capture to be clicked.\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "emotion_labels = ['angry', 'disgust', 'happy', 'sad', 'neutral']  # Ensure this aligns with the model's output\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "model = load_model(model_path)\n",
        "\n",
        "def analyze_emotion(image_path, model):\n",
        "    # Load the original image\n",
        "    face = cv2.imread(image_path)\n",
        "\n",
        "    # Extract face\n",
        "    gray, detected_faces, coord = detect_face(face)\n",
        "    if len(detected_faces) == 0:\n",
        "        print(\"No face detected in the image.\")\n",
        "        return\n",
        "    face_zoom = extract_face_features(gray, detected_faces, coord)\n",
        "\n",
        "    # Convert the extracted face into the format required by the model\n",
        "    img_array = np.reshape(face_zoom[0], (1, 96, 96, 1))\n",
        "\n",
        "    # Model prediction\n",
        "    prediction = model.predict(img_array)[0]  # Get the probabilities for each emotion\n",
        "\n",
        "    # Convert prediction to percentages\n",
        "    prediction_percentages = {emotion_labels[i]: round(prob * 100, 10) for i, prob in enumerate(prediction)}\n",
        "\n",
        "    # Display the original and extracted face\n",
        "    plt.subplot(121)\n",
        "    plt.title(\"Original Face\")\n",
        "    plt.imshow(cv2.cvtColor(face, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.title(\"Extracted Face\")\n",
        "    plt.imshow(face_zoom[0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rWfTl-TkUxN"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "        try:\n",
        "            print(\"Show your happy face! Do not exaggerate and just show your natural happy face.\")\n",
        "            happy = take_photo()\n",
        "            print(\"Show your happy face! Do not exaggerate and just show your natural happy face.\")\n",
        "            filename = take_photo()\n",
        "            print(\"Photo taken! Analyzing your neutral face type...\")\n",
        "\n",
        "            prediction = analyze_emotion(filename, model)\n",
        "\n",
        "            if prediction[3] >= 0.25:\n",
        "                if prediction[0] < 0.15:\n",
        "                    neutral = 'SAD'\n",
        "                    model = 1\n",
        "                else:\n",
        "                    # SAD, angry 동시 충족할 시 기준점에서 떨어진 정도로 결정\n",
        "                    if prediction[0] - 0.15 > prediction[3] - 0.25:\n",
        "                        neutral = 'ANGRY'\n",
        "                        model = 2\n",
        "                    else:\n",
        "                        neutral = 'SAD'\n",
        "                        model = 1\n",
        "            elif prediction[0] >= 0.15:\n",
        "                neutral = 'ANGRY'\n",
        "                model = 2\n",
        "            else:\n",
        "                neutral = 'HAPPY'\n",
        "                model = 3\n",
        "\n",
        "            print(\"Finished analyzing! Now move to the next room and go to the model number\", model)\n",
        "            break  # 성공적으로 실행되었으므로 루프 종료\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"An error occurred:\", str(e))\n",
        "            print(\"Retrying...\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
